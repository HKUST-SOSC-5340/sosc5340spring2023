population <- rbeta (100000, 1, 2)
plot (density (population))
# population mean
mean(population)
# population variance
var(population)
population <- rbeta (100000, 1, 2)
plot (density (population))
# population mean
mean(population)
# population variance
var(population)
survey = sample(population, size = 1000)
plot (density (survey))
# sample mean as estimator of population mean
est.mean = mean(survey)
# unbiased estimator of population variance
est.var = 1000/(1000-1) * var(survey)
survey = sample(population, size = 1000)
plot (density (survey))
# sample mean as estimator of population mean
est.mean = mean(survey)
# unbiased estimator of population variance
est.var = 1000/(1000-1) * var(survey)
est.var
survey_times <- seq(100, 100000, 500)
survey_times
survey_mean_list  <- c()
survey_times <- seq(100, 100000, 500)
for (n in survey_times ){
survey <- sample(population, size = n, replace = F)
survey_mean_list  <- c(survey_mean_list , mean(survey))
}
plot(survey_times, survey_mean_list , xlab = "number of samples", ylab = "sample mean")
abline(h = mean(population), col = "red")
n = 10
survey_mean_list <- c()
for (m in 1:n)
{
survey <- sample(population, size = 1000, replace = TRUE)
survey_mean_list <- c(survey_mean_list, mean(survey))
}
survey_mean_list
n = 10000
survey_mean_list <- c()
for (m in 1:n)
{
survey <- sample(population, size = 1000, replace = TRUE)
survey_mean_list <- c(survey_mean_list, mean(survey))
}
# Now we show the central limit theorem
survey_times <- c(10,  10000)
col <- c("red",  "blue") #"green",
i = 1
for (n in survey_times ){
print (n)
# central limit theorem talks about distribution of the sample mean
# we cannot calculate the distribution for a single survey, so we draw sample multiple times
survey_mean_list <- c()
for (m in 1:n)
{
survey <- sample(population, size = 1000, replace = TRUE)
survey_mean_list <- c(survey_mean_list, mean(survey))
}
# sample_mean_list_standard <- survey_mean_list  - mean(population)
plot(density (survey_mean_list),  col = col[i], xlim = c(0.3, 0.38), ylim  = c(0,60), xlab = "")
abline(v = mean(population), col = "black")
par(new = T)
i = i  + 1
}
sample_mean = mean(sample)
sample_mean = mean(survey)
sample_mean = mean(survey)
#standard error of the sample mean
ss = sqrt(est.var/1000)
print ("point estimate of mean")
print (sample_mean)
print ("95% normal-approximated confidence interval of mean")
c(sample_mean - 1.96 * ss, sample_mean + 1.96 * ss)
length(survey)
length(sample)
length(survey)
print (length(sample))
bootstrap_means = c() # store bootstrapped means
for (i in 1:100){
# resample from the sample with replacement
boot_data <- sample(survey, length(survey), replace = T)
boot_mean <- mean(boot_data)
bootstrap_means <- c(bootstrap_means, boot_mean)
}
bootstrap_means
print (mean(bootstrap_means))
print (sample_mean)
print ("normal confidence interval")
print (c(sample_mean - 1.96 * ss, sample_mean + 1.96 * ss))
plot(density(bootstrap_means), main = "confidence interval")
print (length(sample))
bootstrap_means = c() # store bootstrapped means
for (i in 1:100){
# resample from the sample with replacement
boot_data <- sample(survey, length(survey), replace = T)
boot_mean <- mean(boot_data)
bootstrap_means <- c(bootstrap_means, boot_mean)
}
# then simple quantile function to
print ("point estimate of mean")
print (sample_mean)
print ("point estimate of mean")
print (mean(bootstrap_means))
print ("normal confidence interval")
print (c(sample_mean - 1.96 * ss, sample_mean + 1.96 * ss))
print ("95% bootstrap confidence interval of mean")
conf = quantile(bootstrap_means, c(0.025, 0.975))
print (conf)
plot(density(bootstrap_means), main = "confidence interval")
abline(v = conf[1], col = "red")
abline(v = conf[2], col = "red")
abline(v = sample_mean - 1.96 * ss, col = "green")
abline(v = sample_mean + 1.96 * ss, col = "green")
print (length(sample))
bootstrap_means = c() # store bootstrapped means
for (i in 1:100){
# resample from the sample with replacement
boot_data <- sample(survey, length(survey), replace = T)
boot_mean <- mean(boot_data)
bootstrap_means <- c(bootstrap_means, boot_mean)
}
# then simple quantile function to
print ("point estimate of mean")
print (sample_mean)
print ("point estimate of mean")
print (mean(bootstrap_means))
print ("normal confidence interval")
print (c(sample_mean - 1.96 * ss, sample_mean + 1.96 * ss))
print ("95% bootstrap confidence interval of mean")
conf = quantile(bootstrap_means, c(0.025, 0.975))
print (conf)
plot(density(bootstrap_means), main = "confidence interval")
abline(v = conf[1], col = "red")
abline(v = conf[2], col = "red")
abline(v = sample_mean - 1.96 * ss, col = "green")
abline(v = sample_mean + 1.96 * ss, col = "green")
print (length(sample))
bootstrap_means = c() # store bootstrapped means
for (i in 1:1000){
# resample from the sample with replacement
boot_data <- sample(survey, length(survey), replace = T)
boot_mean <- mean(boot_data)
bootstrap_means <- c(bootstrap_means, boot_mean)
}
# then simple quantile function to
print ("point estimate of mean")
print (sample_mean)
print ("point estimate of mean")
print (mean(bootstrap_means))
print ("normal confidence interval")
print (c(sample_mean - 1.96 * ss, sample_mean + 1.96 * ss))
print ("95% bootstrap confidence interval of mean")
conf = quantile(bootstrap_means, c(0.025, 0.975))
print (conf)
plot(density(bootstrap_means), main = "confidence interval")
abline(v = conf[1], col = "red")
abline(v = conf[2], col = "red")
abline(v = sample_mean - 1.96 * ss, col = "green")
abline(v = sample_mean + 1.96 * ss, col = "green")
# load some required packages
library(ggplot2)
library(margins)
library(ISLR)
library(broom)
library(lmtest)
data(Wage)
data =read.csv("MichelinNY.csv")
l = glm(InMichelin ~ Service + Decor + Food + Price, data, family=binomial("logit"))
d1 <- tidy(coeftest(l))
d1$group <- "R default"
l
d1
# logit^{-1} (X \beta )
invlogit = function(mX, vBeta) {
return(exp(mX %*% vBeta)/(1+ exp(mX %*% vBeta)) )
}
# log-likelihoood function
logLikelihoodLogit = function(vBeta, mX, vY) {
return(- sum(
vY * log(  invlogit(mX, vBeta) ) +
(1-vY)* log(1 - invlogit(mX, vBeta))
)
)
}
vY = as.matrix(data['InMichelin'])
mX = as.matrix(data.frame(`(Intercept)` = 1, data[c('Service','Decor', 'Food', 'Price')]))
vBeta0 = rep(0, ncol(mX))
# optimize
# report every 1 minute
optimLogit <- optim(par = vBeta0,
fn = logLikelihoodLogit,
mX = mX, vY = vY,
method = "BFGS",
hessian=TRUE,
control = list(maxit = 50000, trace = 2, REPORT = 1))
# construct output
coef = optimLogit$par  # coefficient
coef.sd = sqrt(diag(solve(optimLogit$hessian))) # standard error
tv  <- coef  / coef.sd # t-value
## pt is a student-t distribution
## the below line will be the correct way to calculate p-values if you are running linear regression
## but I don't remember whether logistic regression's test statistics follow student-t
pv <- 2 * pt(tv, df = nrow(mX) - ncol(mX), lower.tail = F) # p-value
d = data.frame(term = d1$term, "estimate" = coef,  "std.error" = coef.sd, "statistic" = tv,  "p.value" = pv, check.names = FALSE)
d$group <- "MLE_by_hand"
print (d1)
print (d)
nrow(mX)
ncol(mX)
## pt is a student-t distribution
## the below line will be the correct way to calculate p-values if you are running linear regression
## but I don't remember whether logistic regression's test statistics follow student-t
pv <- 2 * pt(tv, df = nrow(mX) - ncol(mX), lower.tail = F) # p-value
pv
## pt is a student-t distribution
## the below line will be the correct way to calculate p-values if you are running linear regression
## but I don't remember whether logistic regression's test statistics follow student-t
pv <- 2 * pt(-tv, df = nrow(mX) - ncol(mX), lower.tail = F) # p-value
pv
## pt is a student-t distribution
## the below line will be the correct way to calculate p-values if you are running linear regression
## but I don't remember whether logistic regression's test statistics follow student-t
pv <-  pt(-tv, df = nrow(mX) - ncol(mX), lower.tail = F) # p-value
pv
d$group <- "MLE_by_hand"
print (d1)
print (d)
vY = as.matrix(data['InMichelin'])
mX = as.matrix(data.frame(`(Intercept)` = 1, data[c('Service','Decor', 'Food', 'Price')]))
vBeta0 = rep(0, ncol(mX))
# optimize
# report every 1 minute
optimLogit <- optim(par = vBeta0,
fn = logLikelihoodLogit,
mX = mX, vY = vY,
method = "BFGS",
hessian=TRUE,
control = list(maxit = 50000, trace = 2, REPORT = 1))
# construct output
coef = optimLogit$par  # coefficient
coef.sd = sqrt(diag(solve(optimLogit$hessian))) # standard error
tv  <- coef  / coef.sd # t-value
## pt is a student-t distribution
## the below line will be the correct way to calculate p-values if you are running linear regression
## but I don't remember whether logistic regression's test statistics follow student-t
pv <-  pt(-tv, df = nrow(mX) - ncol(mX), lower.tail = F) # p-value
d = data.frame(term = d1$term, "estimate" = coef,  "std.error" = coef.sd, "statistic" = tv,  "p.value" = pv, check.names = FALSE)
d$group <- "MLE_by_hand"
print (d1)
print (d)
vY = as.matrix(data['InMichelin'])
mX = as.matrix(data.frame(`(Intercept)` = 1, data[c('Service','Decor', 'Food', 'Price')]))
vBeta0 = rep(0, ncol(mX))
# optimize
# report every 1 minute
optimLogit <- optim(par = vBeta0,
fn = logLikelihoodLogit,
mX = mX, vY = vY,
method = "BFGS",
hessian=TRUE,
control = list(maxit = 50000, trace = 2, REPORT = 1))
# construct output
coef = optimLogit$par  # coefficient
coef.sd = sqrt(diag(solve(optimLogit$hessian))) # standard error
tv  <- coef  / coef.sd # t-value
## pt is a student-t distribution
## the below line will be the correct way to calculate p-values if you are running linear regression
## but I don't remember whether logistic regression's test statistics follow student-t
pv <-  2 * pt(-tv, df = nrow(mX) - ncol(mX), lower.tail = F) # p-value
d = data.frame(term = d1$term, "estimate" = coef,  "std.error" = coef.sd, "statistic" = tv,  "p.value" = pv, check.names = FALSE)
vY = as.matrix(data['InMichelin'])
mX = as.matrix(data.frame(`(Intercept)` = 1, data[c('Service','Decor', 'Food', 'Price')]))
vBeta0 = rep(0, ncol(mX))
# optimize
# report every 1 minute
optimLogit <- optim(par = vBeta0,
fn = logLikelihoodLogit,
mX = mX, vY = vY,
method = "BFGS",
hessian=TRUE,
control = list(maxit = 50000, trace = 2, REPORT = 1))
# construct output
coef = optimLogit$par  # coefficient
coef.sd = sqrt(diag(solve(optimLogit$hessian))) # standard error
tv  <- coef  / coef.sd # t-value
## pt is a student-t distribution
## the below line will be the correct way to calculate p-values if you are running linear regression
## but I don't remember whether logistic regression's test statistics follow student-t
d = data.frame(term = d1$term, "estimate" = coef,  "std.error" = coef.sd, "statistic" = tv,   check.names = FALSE)
d$group <- "MLE_by_hand"
print (d1)
print (d)
# load some required packages
library(ggplot2)
library(margins)
library(ISLR)
library(broom)
library(lmtest)
data(Wage)
data =read.csv("MichelinNY.csv")
l = glm(InMichelin ~ Service + Decor + Food + Price, data, family=binomial("logit"))
d1 <- tidy(coeftest(l))
d1$group <- "R default"
View(data)
data =read.csv("MichelinNY.csv")
l = glm(InMichelin ~ Service + Decor + Food + Price, data, family=binomial("logit"))
d1 <- tidy(coeftest(l))
d1$group <- "R default"
summary(l)
vY = as.matrix(data['InMichelin'])
head(vY)
mX = as.matrix(data.frame(`(Intercept)` = 1, data[c('Service','Decor', 'Food', 'Price')]))
head(mX)
vBeta0 = rep(0, ncol(mX))
vBeta0
# optimize
# report every 1 minute
optimLogit <- optim(par = vBeta0,
fn = logLikelihoodLogit,
mX = mX, vY = vY,
method = "BFGS",
hessian=TRUE,
control = list(maxit = 50000, trace = 2, REPORT = 1))
# construct output
coef = optimLogit$par  # coefficient
coef
optimLogit$value
logLikelihoodLogit(coef, mX, vY)
coef.sd = sqrt(diag(solve(optimLogit$hessian))) # standard error
tv  <- coef  / coef.sd # t-value
## pt is a student-t distribution
## the below line will be the correct way to calculate p-values if you are running linear regression
## but I don't remember whether logistic regression's test statistics follow student-t
d = data.frame(term = d1$term, "estimate" = coef,  "std.error" = coef.sd, "statistic" = tv,   check.names = FALSE)
l = glm(health ~ wage + education + race, data = Wage, family=binomial("logit"))
summary(l)
head(Wage)
MEM <- margins(l, at = list(wage = mean(Wage$education), race = mean(Wage$race)))
# Marginal effect at representive values
MER <- margins(l, at = list(wage = c(100,200), education = c("2. HS Grad", "3. Some College")))
MER
AME <- margins(l)
AME
cplot(l, "wage", what = "prediction", main = "Predicted probability")
## by default, holding all other to be the constant and vary by focal variable
cplot(l, "race", what = "prediction", main = "Predicted probability")
# use sjPlot
library(sjPlot)
plot_model(l, type = "pred", terms = c("wage", "education"), ci.lvl = NA )
l2 = glm(health ~ wage + education * race, data = Wage, family=binomial())
plot_model(l2, type = "pred", terms = c("wage", "education"))
# load some required packages
library(ggplot2)
library(reshape2)
library(nlme)
library(ISLR)
library(foreign)
library(AER)
library(MASS)
library(tidyverse)
library(ggplot2)
library(knitr)
library(boot)
library(texreg)
library(carData)
data (WVS)
head(WVS)
library(carData)
data (WVS)
head(WVS)
ordered_logit <- polr(poverty~religion+degree+country+age+gender, data = WVS, Hess = TRUE)
summary(ordered_logit)
table(WVS$poverty)
str(WVS$poverty)
ml <- read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")
head(ml)
library(nnet)
multinomial <- multinom(prog ~ ses + write, data = ml)
summary(multinomial)
